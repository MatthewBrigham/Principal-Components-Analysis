{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Principal Component Analysis\n",
    "\n",
    "Matthias Quinn and Matthew Brigham <br>\n",
    "Final Project <br>\n",
    "Cleveland State University <br>\n",
    "MTH 514 <br>\n",
    "Spring 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Principal Components Analysis (PCA) is an unsupervised, dimension-reduction algorithm that was invented by Karl Pearson in 1901.  PCA is commonly used in the areas of computer vision, bioinformatics, finance, and psychology, among others. PCA is particularly helpful because working in lower dimensions can be easier and avoids some of the issues that come with working in higher dimensions. PCA is valuable because it reduces dimensions and minimizes loss of information. PCA is applied to raw data in an attempt to simplify it before it gets analyzed with or entered into a predictive algorithm.\n",
    "\n",
    "PCA aims to take high-dimensional datasets and transform them to a lower dimensional representation. Mathematically, PCA is a linear transformation on a dataset to a new coordinate system of principal components (orthogonal basis vectors). The first coordinate (principal component) will be a projection containing the greatest variance of the data, the second coordinate will contain the second-most amount of variance, and so forth. (Hlavac, n.d.) The principal component(s) with the least amount of variance may be discarded, thereby reducing the dimensions considered. Since each of the remaining principal components are linear combinations of the feature variables, a reduction in dimensions causes minimal loss of original information, making PCA a type of feature extraction. Information is not completely ignored when compared to feature elimination, where the original feature variables would be discarded. \n",
    "\n",
    "There are several benefits to using PCA:\n",
    "\n",
    "* First, working in reduced dimensions can make it easier to visualize data. \n",
    "* Second, PCA is a feature extraction method, therefore minimal information is lost when working in reduced dimensions. \n",
    "* Third, fewer dimensions to consider means that computational time is reduced when running the results through an algorithm, and there is less memory being taken up.  (Shylaja et al, 2011)\n",
    "* Fourth, PCA solves the problem of multicollinearity that can cause unreliable results. Multicollinearity is the phenomenon where two or more independent variables are highly correlated (or co-linear). (<i>Principal Component Analysis Tutorial</i>, n.d.)\n",
    "* Fifth, overfitting is less likely (but still possible) since there are fewer variables. (Brems, 2019)\n",
    "* Sixth, newly created variables are all independent of each other.\n",
    "\n",
    "There are several limitations to consider when using PCA:\n",
    "\n",
    "* First, the newly created variables are linear combinations of all the feature variables and are less interpretable compared to the feature variables.\n",
    "* Second, PCA is sensitive to the differences in scales of the different units among the feature variables. This is usually only an issue if there are large differences in variance among the feature variables. Standardizing the data can be a solution to this issue. (Joliffe, 2002)\n",
    "* Third, PCA does not produce meaningful results on circular, non-linear, and clustered data sets that are largely separated. (Gandluri, 2019) \n",
    "* Fourth, the presence of outliers and uncorrelated data can greatly impact the results of PCA. Their presence in a dataset is problematic, but there are methods to identify and deal with outliers. This sensitivity is a reason why PCA can be used in the detection of outliers. (Joliffe, 2002)\n",
    "\n",
    "\n",
    "There are several assumptions that PCA makes. It is assumed that the variables have a linear relationship; with this assumption, new variables may be created as linear combinations of the features and a new coordinate system can be defined as a system of new basis vectors. Another assumption is that principal components with larger variances are more important than lesser ones. Discarding principal components with small values of variance is analogous to filtering out noise. This assumption helps get cleaner data.  It is also assumed that the principal components are orthogonal and create a basis for a new coordinate system. There are other dimension-reducing techniques that do not make the same assumptions. There exist alternative techniques that circumvent some of these assumptions if PCA cannot be used, for example non-linear PCA (NLPCA) for non-linear datasets.\n",
    "\n",
    "Linear Discriminant Analysis (LDA) is very similar to PCA and both are frequently used in the field of facial recognition for dimension reduction. (Martinez & Kak, 2001) PCA and LDA both reduce the dimensions of the dataset, however, LDA does so by modeling the differences between classes, whereas PCA only models the variance. LDA is a supervised algorithm since it does require the consideration of class to create new variables to maximize class separability. PCA is unsupervised since it does not consider class information, as its primary focus is variance. (Lopes, 2017) In the application to computer vision, it has been shown that for large datasets, LDA can outperform PCA, and PCA can outperform LDA for small datasets. (Martinez & Kak, 2001) PCA can be used at the beginning of an LDA to reduce dimensions, solving the common problem of overfitting in LDA. (amoeba, 2014) There are other dimension reduction techniques than LDA, but it is generally thought that PCA is simpler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Implement PCA\n",
    "\n",
    "One of the primary uses of PCA is dimensionality reduction.  Given a set of data with multiple variables, PCA aims to combine these variables into new variables (or principal components) that are independent and are aligned in the directions of maximal variance. (Nagpal, 2017) The new variables with the least variance may be discarded at the researcher's discretion.  The new variables with the most variance will offer the most information and are kept.  The principal components are linear combinations of all the feature variables, but each differs in the weight of how they represent each feature variable, such that feature variables with low variance have smaller weights. The weights of the linear combinations are the entries in the eigenvectors of the covariance matrix. Even though some variables will be dropped by the end, all the feature variables can still contribute information to the final calculation. Thus, the loss of information is minimized when working in these reduced dimensions. \n",
    "\n",
    "PCA can be solved using eigen decomposition or singular value decomposition (SVD), both being very similar in methodology. The two differ in the calculation of the change of basis, but yield similar results. We show eigen decomposition in the methods below. (Shlens, 2005)\n",
    "\n",
    "1\\. Standardize the data about the mean\n",
    "\n",
    "* The first step is to standardize the dataset, defined as $X_{standard}$.  Each feature will have its own mean and standard deviation; for comparison purposes, standardizing is an essential first step. The formula below is used to standardize a set of data of $n$ features with $m$ many data points per feature organized into an $m$ x $n$ matrix.  Note that the respective mean and standard deviation for each column can be different than other columns and are used only to standardize each sample point in their respective column, $X_{i}$.  It follows that $X_{standard}$ is also $m$ x $n$.\n",
    "   \n",
    "\\begin{align*}\n",
    "X_{standard} = \\frac{(X_{i} - \\bar{X})}{std(X)}\n",
    "\\end{align*}\n",
    "\n",
    "2\\. Compute the covariance matrix\n",
    "\n",
    "* A covariance matrix shows how each variable in the data set co-varies with every other variable.  The covariance matrix will be a symmetric $n$ x $n$ matrix. \n",
    "\n",
    "\n",
    "\\begin{align*}   \n",
    "C = \\frac{1}{n-1} \\sum^{n}_{i=1}{(X_i-\\bar{X})(X_i-\\bar{X})^T}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "3\\. Compute the eigen decomposition of the covariance matrix\n",
    "\n",
    "* The eigen decomposition yields the eigenvalues and eigenvectors of the covariance matrix.  The eigenvalues can be calculated using the formula below.  There are at most $n$-many eigenvalues and eigenvectors. Note that since the covariance matrix is symmetric, there will always be $n$-many real eigenvalues and eigenvectors. (Fitzpatrick, 2011) The eigenvectors of the covariance matrix provide information on the directions in which the data is oriented (variance). The eigenvalue tells the length of the eigenvector, or the amount of variance in that direction. \n",
    "\n",
    "\\begin{align*}\n",
    "det(\\lambda I - C) = 0\n",
    "\\end{align*}\n",
    "\n",
    "* The following formula can be solved to obtain the eigenvectors $V_{1}$,...,$V_{k}$,...,$V_{n}$. The eigenvectors of the covariance matrix are the principal components and basis vectors in the new coordinate system.   \n",
    "\n",
    "\\begin{align*}\n",
    "(\\lambda I - C)V_{k} = 0\n",
    "\\end{align*}\n",
    "\n",
    "4\\. Choose a set of $k$-many eigenvectors and sort the eigenvectors by corresponding decreasing eigenvalues. \n",
    "\n",
    "* Variables with more variance tell more information about the data than variables with less variance, which is one of the assumptions of PCA. The principal components with the smallest eigenvalues are typically discarded. The selected eigenvectors can then be organized into an $n$ x $k$ eigenvector matrix, where the vectors are sorted in decreasing order according to their corresponding eigenvalues. The decision on how many principal components to keep is up to the researcher. A desired number of dimensions to work in or a desired percentage of variance explained can be used to determine how many principal components to keep or discard. A scree plot can be useful to help make this decision.\n",
    " \n",
    "5\\. Use the $ n $ x $ k $ eigenvector matrix to transform the samples into the new subspace\n",
    "\n",
    "* Once the $k$-many eigenvectors are chosen, the $n$ x $k$ eigenvector matrix $E= [V_{1} ... V_{K}]$  can be constructed where $1 \\le k < n$.  The original data can be transformed into the new coordinate system using the formula below, denoted $X_{transformed}$. The tansformed  vector matrix will be of size $m$ x $k$. \n",
    "\n",
    "\\begin{align*}\n",
    "X_{transformed} = X_{standard}*E\n",
    "\\end{align*}\n",
    "\n",
    "Standardization can be important when the units of each feature are different. Different units may scale differently, causing some variances to be substantially larger or smaller than others. This could affect the weight each principal component assigns to feature variables and less important variables (noise) may be over-represented in the principal components. (Pedregosa et al.) It is important that the variances are not too significantly different. If some variances are substantially larger than others among the feature variables, then the linear combination constructing the first principal component will be dominated by the feature variable with the largest variance,  the second principal component dominated by the variable with the second largest variance, and so on.  The result is that the principal components are not that different than from the feature variables, or may inaccurately portray the importance of the feature variables. Standardizing the data or using a correlation matrix can correct this issue. It is also recommended to standardize when PCA is going to be paired with a machine learning algorithm when the features have different units or scale. (Lakshmanan, 2019)\n",
    "\n",
    "It is not always advisable to standardize the data. The decision whether to leave the data raw, centered, standardized or not should be considered. If all the variables have the same units and the variance of each variable is close in value to the others, a covariance matrix can yield better results than a correlation matrix.  It is recommended that data should not be centered when working with heterogeneous data and identifying clusters is desirable. Data centering is recommended when the data is homogeneous. Variables with large variances can dominate the principal components (as stated previously), and in these cases, it is recommended to try standardizing the data. (Hinch & Somers, 1987) Standardizing can solve the issue of scale when the variables have different units or substantially different variances. In most cases, especially when the data will be run through a machine learning or predictive algorithm, it is recommended to standardize the data. The cases in which it is beneficial to not standardize are special cases.\n",
    "\n",
    "Based on the data, it is important to decide whether it is appropriate to use a covariance matrix versus a correlation matrix.  Correlation matrices are used more commonly than covariance matrices because the results are more comparable, since the data is standardized. There are cases where a covariance matrix can yield better results than the correlation matrix--without standardizing. Standardizing data in the correlation matrix or before using the covariance matrix can lead to some loss of information because standardization involves equating the variances among the variables. (<i>Principal Component Analysis</i>, (n.d.); Hinch & Somers, 1987)  If the units are the same for the variables and the variances of each feature are close, then the covariance matrix can be easier to use and yield better results, making it preferable to the correlation matrix. Correlation matrices are commonly used in practice when the data has large variances between each other, or if the scale of variables differ due to different units. The problem of scale for covariance matrices is generally combatted by standardizing the data, which can yield similar results to using a correlation matrix. Therefore, depending on the data and application, choice of matrix can be made at the researcher's discretion. The method described above uses the covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application: Fisher's Iris Dataset\n",
    "\n",
    "Fisher's iris dataset is a famous dataset collected by the statistician Ronald Fisher. (Fisher, 1936) The dataset consists of 50 samples of each of three iris species, namely: Setosa, Versicolor, and Virginica.  The goal of Fisher's analysis was to classify each of the three species using a flower's following measurements: sepal length, sepal width, petal length, and petal width. \n",
    "\n",
    "Fisher attempted to develop a linear discriminant model to distinguish between the three iris species.  Like PCA, this model attempted to reduce dimensionality, however, it did so by trying to maximize class separability.  PCA is different in that it attempts to maximize variance and does not consider differences among features. PCA can yield better results than LDA when the training set is small. (Martinez & Kak, n.d.)\n",
    "\n",
    "This application utilizes the same data set that Fisher used in his paper. Since the features are of similar units, scale, and variances, a non-standardized covariance matrix could yield a similar or better results than the correlation matrix. However, it is common to standardize before being put into machine learning algorithms, so for this reason the data was standardized. The dataset is 150 rows (indexed from 0 to 149). The first 50 rows are measurements for the iris setosa, the next 50 rows are the iris versicolor, and the final 50 rows are the iris virginica. Table 1 shows the first four rows of the dataset. The code used for this application is shown in Appendix A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLength  SepalWidth  PetalLength  PetalWidth        Class\n",
       "0          5.1         3.5          1.4         0.2  Iris-setosa\n",
       "1          4.9         3.0          1.4         0.2  Iris-setosa\n",
       "2          4.7         3.2          1.3         0.2  Iris-setosa\n",
       "3          4.6         3.1          1.5         0.2  Iris-setosa"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "\n",
    "\n",
    "# Import the Iris dataset\n",
    "\n",
    "names = [\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\", \"Class\"]\n",
    "iris = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',\n",
    "       names=names,\n",
    "       sep=',')\n",
    "\n",
    "# Separate labels from data\n",
    "X = iris.iloc[:, 0:4].values # Select the first four coulmns\n",
    "y = iris.iloc[:, -1].values   # Select the labels\n",
    "\n",
    "\n",
    "df1 = iris[0:4]\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Table 1:  The iris dataset shows data collected on three different species of iris.  There are fifty unique plants/samples of each species measured.  Four features were measured for each sample: sepal length, sepal width, petal length, petal width. All measurements were made in centimeters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire data set was standardized before the covariance matrix was calculated. The covariance matrix is shown below.  Notice that the covariance matrix is symmetric.  The eigenvalues of the covariance matrix are also shown and have been sorted in descending order.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrix: \n",
      "[[ 1.00671141 -0.11010327  0.87760486  0.82344326]\n",
      " [-0.11010327  1.00671141 -0.42333835 -0.358937  ]\n",
      " [ 0.87760486 -0.42333835  1.00671141  0.96921855]\n",
      " [ 0.82344326 -0.358937    0.96921855  1.00671141]]\n"
     ]
    }
   ],
   "source": [
    "X_std = (X - np.mean(X, axis = 0)) / np.std(X, axis = 0)\n",
    "mean = np.mean(X_std, axis = 0) # column-wise means\n",
    "cov_mat = (X_std - mean).T.dot((X_std - mean)) / (X_std.shape[0] - 1)\n",
    "print(\"Covariance matrix: \")\n",
    "print(cov_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Eigenvalues of the Covariance Matrix:\n",
      "2.9303537755893156\n",
      "0.9274036215173419\n",
      "0.14834222648164008\n",
      "0.02074601399559599\n"
     ]
    }
   ],
   "source": [
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "eig_pairs.sort(key = lambda x: x[0], reverse=True)\n",
    "print(\"Sorted Eigenvalues of the Covariance Matrix:\")\n",
    "for i in eig_pairs: # List eigenvalues in descending order\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a total of four principal components that can be derived from this set of four eigenvalues.  The next step will be to determine which principal components contribute the most variance and which can be discarded. This can be visualized with the help of a scree plot, shown in Figure 1. Figure 1 graphically shows the percentage of variance each principal component contributes to the data. An appropriate number of components to use is the first two because they account for almost 96% of the total variation in the dataset.  Minimal information is lost when the third and fourth principal components are discarded.  Remember that the first and second principal components are linear combinations of all the iris features, therefore, each feature is still being represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://drive.google.com/uc?id=17HJiq_aCGXnBRZLV2WCBjpPBO_UTNMK5\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://drive.google.com/uc?id=17HJiq_aCGXnBRZLV2WCBjpPBO_UTNMK5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Figure 1: The scree plot above shows the variance each principal component contributes to the total variance.  Selecting the first two principal components would account for about 95.9% of the total variance.  Since the third and fourth principal components contribute relatively little information, they can be discarded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot of the transformed data in Figure 2 shows how the data is clustered based on the information from the original dataset. One of the benefits of PCA is it can be easier to visualize data. The original dataset had four dimensions and was reduced to a two-dimensional problem that could be plotted.  The plot shows that it should be easy to distinguish the iris setosa from the other species because the data is clearly clustered away from the other data points. There may be some issues differentiating between versicolor and virginica since they were not completely separable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://drive.google.com/uc?id=1HiY2Fvnr_G9bC4qvVMaLtslkrGm1bHzU\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://drive.google.com/uc?id=1HiY2Fvnr_G9bC4qvVMaLtslkrGm1bHzU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Figure 2: One benefit of having two dimensions is that the data can now be visualized graphically. The iris setosa is distinguishable from the versicolor and virginica. An algorithm that determines species based on measurements could struggle with iris versicolor and virginica since the data is not as separable as the setosa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application II: Olivetti Image Dataset\n",
    "\n",
    "In the early 1990s, AT&T commissioned research on facial recognition by the Olivetti Research Laboratory in Cambridge.  The dataset they used contained a total of 400 images, consisting of 10 different images for each of 40 people.  Each image differed in lighting, angle, facial expression, and facial details (glasses or no glasses).  The images are colored on a 256-level grayscale. The structure of the dataset is a matrix of 400 rows by 4096 columns.  Each row represents one facial image.  Each image is 64x64 pixels (a total of 4096 pixels per image); therefore, each column value is a variable that represents a greyscale color for that pixel (scaled to a range from 0 to 1). The original images are shown in Figure 3. (<i>The Database of Faces</i>, 2001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://drive.google.com/uc?id=1SPd1vOvZrOzCn1Nl9CiB-eOvEvwQh-IG\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://drive.google.com/uc?id=1SPd1vOvZrOzCn1Nl9CiB-eOvEvwQh-IG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Figure 3: The first 32 samples in the dataset are shown. There are ten images per person taken at different angles, lighting, and with different facial expressions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many variables in this dataset (4096 variables), therefore, a dimension reduction technique such as PCA or LDA will be useful. PCA will likely outperform LDA because there is a small sample of images per class, or person. (Martinez & Kak, 2001) The goal of this application will be to compare the original images of the dataset to the transformed images--called eigenfaces. Each principal component derived from the covariance matrix produces an eigenface. Each principal component represents only a small portion of a face, not necessarily a facial feature, such as a nose or mouth. When combined with the correct weights, a face from the data set can be constructed by a linear combination of eigenfaces/principal components. (Turk & Pentland, 1991) By reducing the complexity, it will be shown that computation times improve. In practice, PCA could be applied to the dataset before being ran through a classifier to improve its efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying PCA to the dataset, it is shown that similar results can be achieved using only 64 variables instead of 4096 while retaining 89.7% of the variance. The final image results are shown in Figure 4. The scree plot in Figure 5 shows how much variance each principal component contributes. Instead of a 64x64 image, an easier 8x8 image could be used that has far fewer variables and data points. The code for this application uses a pre-defined PCA function that utilizes SVD instead of eigenvector decomposition. The results of PCA on the Olivetti dataset suggest how important dimension reduction techniques can be. Complex algorithms would benefit greatly from working with fewer data points.  The code used is shown in Appendix B.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://drive.google.com/uc?id=1WL9IH-Gog7DK4m198Q9xpwDwpd2MdaLj\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://drive.google.com/uc?id=1WL9IH-Gog7DK4m198Q9xpwDwpd2MdaLj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Figure 4: After PCA was performed, the faces are still recognizable and retained about 89.7% of the total variance. Even thought there was a loss in image quality, there are significantly fewer dimensions and a large amount of the total variance was still retained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://drive.google.com/uc?id=1uXrCVCRegrtmaGbv0FzQCmeY1IkMDX_f\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://drive.google.com/uc?id=1uXrCVCRegrtmaGbv0FzQCmeY1IkMDX_f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Figure 5: The scree plot shows how much variance each of the first ten principal components shows.  A total of sixty-four components were used in the calculations to obtain the results in Figure 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once PCA is applied, the data can be entered and analyzed with a facial recognition classifier. One noteworthy benefit of PCA is that it can help improve the computational speed by reducing the amount of data to be processed with a minimal increase in classification error. A random forest model is one type of algorithm that can be used for facial recognition and was used to analyze the time benefits. The code used to analyze the computation speed is in Appendix C and only analyzed the time to model the data with a random forest and did not classify faces. It should be noted that computational speed is also dependent on the processor used in the computer. Regardless, the improvement is still clear: on a Ryzen 7 3700X processor, it took 1.706 seconds to model the original data and 0.272 seconds to model the transformed data (a 527% increase in speed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "PCA is a simple, proven dimension reduction technique that can help improve the analysis of data.  Being an unsupervised algorithm, it does benefit from some consideration for the data before blindly running it.  PCA does not always simplify data analysis and does not always yield good results.  There are alternative dimension reducing techniques that may be better suited for the cases where PCA fails. All of that being said, PCA can still be a great tool to reduce the complexity of a dataset before analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hlaváč, V. (n.d.). Prague. Retrieved from http://people.ciirc.cvut.cz/~hlavac/TeachPresEn/11ImageProc/15PCA.pdf\n",
    "\n",
    "Shlens, J. (2005). A Tutorial on Principal Component Analysis. Retrieved from https://arxiv.org/pdf/1404.1100.pdf\n",
    "\n",
    "Shylaja S. Sharath, Balasubramanya Murthy K N and Natarajan S (July 27th 2011). Dimensionality Reduction Techniques \n",
    "    for Face Recognition, Reviews, Refinements and New Ideas in Face Recognition, Peter M. Corcoran, IntechOpen, DOI: \n",
    "    10.5772/18251. Available from: https://www.intechopen.com/books/reviews-refinements-and-new-ideas-in-face-\n",
    "    recognition/dimensionality-reduction-techniques-for-face-recognition\n",
    "\n",
    "Multicollinearity. (n.d.). Retrieved from https://www.statisticssolutions.com/multicollinearity/\n",
    "\n",
    "Principal Component Analysis Tutorial. (n.d.). Retrieved from https://www.dezyre.com/data-science-in-python-\n",
    "    tutorial/principal-component-analysis-tutorial\n",
    "\n",
    "Brems, M. (2019, June 10). A One-Stop Shop for Principal Component Analysis. Retrieved from \n",
    "    https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c\n",
    "\n",
    "Jolliffe, I. T. (2002). Principal component analysis (2nd ed.). New York: Springer.\n",
    "\n",
    "Gandluri, S. (2019, July 20). Principal Component Analysis(PCA) in Machine Learning. Retrieved from \n",
    "    https://medium.com/@shiva1gandluri/principal-component-analysis-pca-in-machine-learning-c3f239249b73\n",
    "\n",
    "Lopes, M. (2017, January 27). Dimensionality Reduction - Does PCA really improve classification outcome? Retrieved \n",
    "    from https://towardsdatascience.com/dimensionality-reduction-does-pca-really-improve-classification-outcome-\n",
    "    6e9ba21f0a32\n",
    "\n",
    "Martinez, A. M., & Kak, A. C. (2001). PCA versus LDA. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(2), 228–233. doi: 10.1109/34.908974. Retrieved from http://www2.ece.ohio-state.edu/~aleix/pami01.pdf \n",
    "\n",
    "amoeba. (2014, August). Does it make sense to combine PCA and LDA? Retrieved from \n",
    "    https://stats.stackexchange.com/questions/106121/does-it-make-sense-to-combine-pca-and-lda\n",
    "\n",
    "Nagpal, A. (2017, November 21). Retrieved from https://towardsdatascience.com/principal-component-analysis-intro-\n",
    "    61f236064b38\n",
    "\n",
    "Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011. Retrieved from \n",
    "    https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html\n",
    "\n",
    "Lakshmanan, S. (2019, May 17). How, When and Why Should You Normalize/Standardize/Rescale Your Data? Retrieved from \n",
    "    https://medium.com/@swethalakshmanan14/how-when-and-why-should-you-normalize-standardize-rescale-your-data-\n",
    "    3f083def38ff\n",
    "\n",
    "Hinch, S., & Somers, K. (1987). AN EXPERIMENTAL EVALUATION OF THE EFFECT OF DATA CENTERING, DATA STANDARDIZATION, AND \n",
    "    OUTLYING OBSERVATIONS ON PRINCIPAL COMPONENT ANALYSIS. Coenoses, 2(1), 19-23. www.jstor.org/stable/43460941\n",
    "\n",
    "Principal Component Analysis. (n.d.). Retrieved from https://www.originlab.com/doc/Origin-Help/PrincipleComp-Analysis\n",
    "\n",
    "Fitzpatrick, R. (2011, March 31). Retrieved from \n",
    "    http://farside.ph.utexas.edu/teaching/336k/Newtonhtml/Newtonhtml.html\n",
    "\n",
    "Overfitting in Machine Learning: What it is and How to Prevent it. (n.d.). Retrieved from \n",
    "    https://elitedatascience.com/overfitting-in-machine-learning#signal-vs-noise\n",
    "\n",
    "Fisher, R. A. (1936). The Use Of Multiple Measurements In Taxonomic Problems. Annals of Eugenics, 7(2), 179–188. doi: \n",
    "    10.1111/j.1469-1809.1936.tb02137.x\n",
    "\n",
    "The Database of Faces. (n.d.). Retrieved from http://cam-orl.co.uk/facedatabase.html/\n",
    "\n",
    "Database of Faces. AT&T Laboratories Cambridge. (2001). zip. Cambridge.\n",
    "\n",
    "Walia, A. S. (2018, February 15). Principal Component Analysis – Unsupervised Learning. Retrieved from \n",
    "    https://datascienceplus.com/principal-component-analysis-unsupervised-learning/\n",
    "\n",
    "Principal Components Analysis. (n.d.). The University of Bridgeport. \n",
    "    http://kiwi.bridgeport.edu/cpeg540/PrincipalComponentAnalysis_Tutorial.pdf\n",
    "\n",
    "Dubey, A. (2018, December 25). The Mathematics Behind Principal Component Analysis. Retrieved from \n",
    "    https://towardsdatascience.com/the-mathematics-behind-principal-component-analysis-fff2d7f4b643\n",
    "\n",
    "Brownlee, J. (2019, August 9). How to Calculate Principal Component Analysis (PCA) from Scratch in Python. Retrieved \n",
    "    from https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/\n",
    "\n",
    "VanderPlas, J. (2016). Machine Learning. In Python Data Science Handbook. Beijing: O'Reilly Media. Retrieved from \n",
    "    https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\n",
    "\n",
    "Giampieri, E. (2012, December). Principal Component Analysis (PCA) in Python. Retrieved from \n",
    "    https://stackoverflow.com/questions/13224362/principal-component-analysis-pca-in-python\n",
    "\n",
    "Eigenvalues: Quick data visualization with factoextra - R software and data mining. (n.d.). Retrieved from \n",
    "    http://www.sthda.com/english/wiki/eigenvalues-quick-data-visualization-with-factoextra-r-software-and-data-mining\n",
    "\n",
    "Fisher, R. A. (1936). Iris Dataset. Retrieved from https://archive.ics.uci.edu/ml/datasets/iris\n",
    "\n",
    "R Core Team. (2016). R: A Language and Environment for Statistical Computing. Vienna, Austria. Retrieved from \n",
    "    https://www.R-project.org/\n",
    "\n",
    "Turk, M., & Pentland, A. (1991). Face recognition using eigenfaces. IEEE Computer Society Conference on Computer \n",
    "    Vision and Pattern Recognition. doi: 10.1109/cvpr.1991.139758. Retrieved from \n",
    "    https://sites.cs.ucsb.edu/~mturk/Papers/mturk-CVPR91.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix A - Application I Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code applies PCA to the Iris Dataset collect by R. A. Fisher in 1936. \n",
    "# PCA turns the 4-D dataset into a 2-D dataset that can be visualized easily.\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "#----Import the Iris dataset and Separate Labels from Data-----#\n",
    "\n",
    "names = [\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\", \"Class\"]\n",
    "iris = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data',\n",
    "       names=names,\n",
    "       sep=',')\n",
    "\n",
    "X = iris.iloc[:, 0:4].values  # Select the first four coulmns\n",
    "y = iris.iloc[:, -1].values   # Select the labels\n",
    "\n",
    "#df1 = iris[0:4]    \n",
    "#display(df1)       #shows first 4 rows of dataset\n",
    "\n",
    "#--------------------Standardize Data---------------------#\n",
    "\n",
    "X_std = (X - np.mean(X, axis = 0)) / np.std(X, axis = 0)\n",
    "mean = np.mean(X_std, axis = 0)   # column-wise means\n",
    "\n",
    "#----------------Compute Covariance Matrix----------------#\n",
    "\n",
    "cov_mat = (X_std - mean).T.dot((X_std - mean)) / (X_std.shape[0] - 1)\n",
    "print(\"Covariance matrix: \")\n",
    "print(cov_mat)\n",
    "\n",
    "#-----------Compute Eigenvectors and Eigenvalues----------#\n",
    "\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "eig_pairs.sort(key = lambda x: x[0], reverse=True)\n",
    "print(\"Sorted Eigenvalues of the Covariance Matrix:\")\n",
    "for i in eig_pairs: # List eigenvalues in descending order\n",
    "    print(i[0])\n",
    "\n",
    "#---------------Choose 2 Principal Components--------------#    \n",
    "\n",
    "matrix = np.hstack((eig_pairs[0][1].reshape(4, 1),\n",
    "                    eig_pairs[1][1].reshape(4, 1)))\n",
    "\n",
    "#-------Transform Standardized Data into New Subspace------#\n",
    "\n",
    "Y = X_std.dot(matrix)\n",
    "# Y[:10]\n",
    "\n",
    "#---------------------Plot New Dataset---------------------#\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "for lab, col in zip(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'),\n",
    "                    ('blue', 'red', 'green')):\n",
    "    plt.scatter(Y[y==lab, 0],\n",
    "                Y[y==lab, 1],\n",
    "                label=lab,\n",
    "                c=col)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title(\"Principal Components Analysis on Iris Flowers\")\n",
    "plt.legend(loc='lower center')\n",
    "plt.tight_layout()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix B - Application II Code  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code applies PCA to the Olivetti Dataset of Faces. It takes a dataset of \n",
    "# 4096 variables and reduces the dimensions to 64, with minimal loss of information.\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from sklearn.datasets import fetch_openml\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "\n",
    "olive = fetch_olivetti_faces()\n",
    "#dir(olive)\n",
    "print(f\"Shape of original Olivetti Dataset: {olive.data.shape}\")  #prints the shape of the original dataset\n",
    "\n",
    "#-------Plot 32 Original Sample Images--------#\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "fig.subplots_adjust(left = 0, right = 1, bottom = 0, top = 1, hspace = 0.5, wspace = 0.05)\n",
    "\n",
    "\n",
    "for i in range(32): \n",
    "  ax = fig.add_subplot(8, 8, i+1, xticks = [], yticks = [])\n",
    "  ax.imshow(olive.images[i], cmap = plt.cm.bone, interpolation = \"nearest\")\n",
    "\n",
    "#------Apply PCA to the original dataset------#\n",
    "\n",
    "X,y = olive.data, olive.target\n",
    "pca = PCA(n_components=64)  # This PCA function uses SVD instead of eigendecomposition \n",
    "X_trans = pca.fit_transform(X)\n",
    "print(f\"Shape of Transformed Dataset: {X_trans.shape}\")\n",
    "def sum(list):                   # Input is a list of numbers\n",
    "    sum = 0                      # Initially start the sum at 0\n",
    "    for i in range(0,len(list)): # For each number in the list:\n",
    "        num = list[i]            # Take the ith number in the list as input\n",
    "        sum += num               # Add the ith number to the current sum\n",
    "    return sum                   # Return the scalar sum\n",
    "\n",
    "print(f\"Total Variance Explained: {sum(pca.explained_variance_ratio_)*100}%\")  \n",
    "\n",
    "#---Plot Transformed Dataset Sample Images---#\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "fig.subplots_adjust(left=0, right = 1, bottom = 0, top = 1, hspace = .05, wspace = 0.05)\n",
    "\n",
    "for i in range(10):    #show 10 transformed images\n",
    "   ax = fig.add_subplot(5, 5, i+1, xticks=[], yticks=[]) \n",
    "   ax.imshow(np.reshape(pca.components_[i,:], (64,64)), cmap=plt.cm.bone, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix C - Time Analysis of Olivetti Dataset with Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code attempts to quantify the time benefit that working\n",
    "# in reduced dimensions offers. The original dataset is entered into\n",
    "# a random forest classifier and timed. The transformed, smaller dataset \n",
    "# that PCA was applied to is also entered into a random forest classifier\n",
    "# and timed. This code shows that working in smaller dimensions can improve\n",
    "# the computational speed of algorithms.\n",
    "\n",
    "import timeit\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "olive = fetch_olivetti_faces()\n",
    "dir(olive)\n",
    "\n",
    "X,y = olive.data, olive.target\n",
    "pca = PCA(n_components=64)\n",
    "X_trans = pca.fit_transform(X)\n",
    "print(X_trans.shape)\n",
    "\n",
    "# Create time function for Random Forest model:\n",
    "\n",
    "def ModelTime(X, y):\n",
    "    start = timeit.default_timer()\n",
    "    RandomForestClassifier().fit(X=X, y=y)\n",
    "    stop = timeit.default_timer()\n",
    "    print(f\"Time {stop - start}\")\n",
    "\n",
    "# Model 1 - Original data set time of computation\n",
    "\n",
    "ModelTime(X=X, y=y)\n",
    "\n",
    "# Model 2 - PCA data set time of computation\n",
    "\n",
    "ModelTime(X=X_trans, y=y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
